{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRaNUJmY7Jlzdpsp3TNDaQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrishikesht7/CropRecommendationSystem/blob/main/CropRecommendationSystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIPQ2Qbttf8S",
        "outputId": "e8f7046e-3a57-4b49-9021-221b29a00d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'crop-recommendation-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/crop-recommendation-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"atharvaingle/crop-recommendation-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlWK4Xj-xcm1",
        "outputId": "113f8782-f226-4586-8fa1-95ab186927fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "cSQyFBBMx7q5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kagglehub pandas numpy scikit-learn torch xgboost lightgbm joblib seaborn matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH10Ym5b0XzP",
        "outputId": "0e7ba0e2-54e7-413e-ca05-a1d739c815ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
        "import joblib\n",
        "from scipy.stats import randint, uniform\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress warnings\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# 1. Load and preprocess\n",
        "path = kagglehub.dataset_download(\"atharvaingle/crop-recommendation-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "data = pd.read_csv(f\"{path}/Crop_recommendation.csv\")\n",
        "X = data.drop('label', axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "X_poly = poly.fit_transform(X)\n",
        "X = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=20)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Keep feature names for LightGBM\n",
        "selected_features = X_train.columns[selector.get_support()].tolist()\n",
        "X_train_selected = pd.DataFrame(X_train_selected, columns=selected_features)\n",
        "X_test_selected = pd.DataFrame(X_test_selected, columns=selected_features)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Save preprocessors\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "joblib.dump(poly, 'poly_features.pkl')\n",
        "joblib.dump(selector, 'feature_selector.pkl')\n",
        "\n",
        "print(f\"Features after selection: {X_train_scaled.shape[1]}\")\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "num_classes = len(label_encoder.classes_)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# 2. LeNet-inspired\n",
        "class LeNetInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(LeNetInspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 16, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)\n",
        "        self.fc1 = nn.Linear(32, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.pool = nn.MaxPool1d(1)  # Minimal pooling for tabular data\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "lenet_model = LeNetInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 3. GoogLeNet-inspired\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        self.branch1 = nn.Conv1d(in_channels, out_channels // 4, kernel_size=1)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels // 4, kernel_size=1),\n",
        "            nn.Conv1d(out_channels // 4, out_channels // 4, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels // 4, kernel_size=1),\n",
        "            nn.Conv1d(out_channels // 4, out_channels // 4, kernel_size=5, padding=2)\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=1),\n",
        "            nn.Conv1d(in_channels, out_channels // 4, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "        return torch.cat([branch1, branch2, branch3, branch4], dim=1)\n",
        "\n",
        "class GoogLeNetInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(GoogLeNetInspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.inception1 = InceptionModule(64, 128)\n",
        "        self.inception2 = InceptionModule(128, 128)\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "googlenet_model = GoogLeNetInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 4. AlexNet-inspired\n",
        "class AlexNetInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(AlexNetInspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 96, kernel_size=11, stride=1, padding=5)\n",
        "        self.conv2 = nn.Conv1d(96, 256, kernel_size=5, padding=2)\n",
        "        self.fc1 = nn.Linear(256, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.pool = nn.MaxPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "alexnet_model = AlexNetInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 5. Channel-Boosted CNN\n",
        "class ChannelBoostedCNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ChannelBoostedCNN, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(512, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.pool = nn.MaxPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "channel_boosted_model = ChannelBoostedCNN(input_dim, num_classes).to(device)\n",
        "\n",
        "# 6. Inception V2-inspired\n",
        "class InceptionV2Module(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionV2Module, self).__init__()\n",
        "        self.branch1 = nn.Conv1d(in_channels, out_channels // 4, kernel_size=1)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels // 8, kernel_size=1),\n",
        "            nn.Conv1d(out_channels // 8, out_channels // 4, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels // 8, kernel_size=1),\n",
        "            nn.Conv1d(out_channels // 8, out_channels // 4, kernel_size=3, padding=1),\n",
        "            nn.Conv1d(out_channels // 4, out_channels // 4, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=1),\n",
        "            nn.Conv1d(in_channels, out_channels // 4, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "        return torch.cat([branch1, branch2, branch3, branch4], dim=1)\n",
        "\n",
        "class InceptionV2InspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(InceptionV2InspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.inception1 = InceptionV2Module(64, 128)\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.inception1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "inception_v2_model = InceptionV2InspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 7. Inception V3-inspired\n",
        "class InceptionV3InspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(InceptionV3InspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.inception1 = InceptionV2Module(64, 128)\n",
        "        self.inception2 = InceptionV2Module(128, 256)\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "inception_v3_model = InceptionV3InspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 8. Inception V4-inspired\n",
        "class InceptionV4InspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(InceptionV4InspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.inception1 = InceptionV2Module(64, 128)\n",
        "        self.inception2 = InceptionV2Module(128, 256)\n",
        "        self.inception3 = InceptionV2Module(256, 384)\n",
        "        self.fc1 = nn.Linear(384, 192)\n",
        "        self.fc2 = nn.Linear(192, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = self.inception3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "inception_v4_model = InceptionV4InspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 9. ResNeXt-inspired\n",
        "class ResNeXtBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, cardinality=4):\n",
        "        super(ResNeXtBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, groups=cardinality)\n",
        "        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size=1)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
        "        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x = x + shortcut\n",
        "        return F.relu(x)\n",
        "\n",
        "class ResNeXtInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ResNeXtInspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.block1 = ResNeXtBlock(64, 128)\n",
        "        self.block2 = ResNeXtBlock(128, 128)\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "resnext_model = ResNeXtInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 10. DenseNet-inspired\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, growth_rate, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(growth_rate)\n",
        "        self.conv2 = nn.Conv1d(in_channels + growth_rate, growth_rate, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(growth_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = torch.cat([x, out1], dim=1)\n",
        "        out2 = F.relu(self.bn2(self.conv2(out)))\n",
        "        return torch.cat([x, out1, out2], dim=1)\n",
        "\n",
        "class DenseNetInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(DenseNetInspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.dense1 = DenseBlock(64, 32)\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dense1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "densenet_model = DenseNetInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 11. VGG-inspired\n",
        "class VGGInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(VGGInspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.pool = nn.MaxPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "vgg_model = VGGInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 12. Transformer-inspired\n",
        "class TransformerInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=128, num_heads=4, num_layers=2):\n",
        "        super(TransformerInspiredModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).unsqueeze(1) + self.pos_embed\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "transformer_model = TransformerInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 13. EfficientNet-inspired\n",
        "class EfficientNetInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(EfficientNetInspiredModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=128, num_heads=4, batch_first=True)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.fc4 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.swish(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.swish(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        attn_output, _ = self.attention(x.unsqueeze(1), x.unsqueeze(1), x.unsqueeze(1))\n",
        "        x = x + attn_output.squeeze(1)\n",
        "        x = F.layer_norm(x, x.shape[1:])\n",
        "        x = self.swish(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc4(x)\n",
        "\n",
        "effnet_model = EfficientNetInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 14. ConvNeXt-inspired\n",
        "class ConvNeXtInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ConvNeXtInspiredModel, self).__init__()\n",
        "        self.reshape = lambda x: x.view(-1, input_dim, 1)\n",
        "        self.conv1 = nn.Conv1d(input_dim, 40, kernel_size=7, padding=3, groups=input_dim)\n",
        "        self.conv2 = nn.Conv1d(40, 40, kernel_size=1)\n",
        "        self.ln1 = nn.LayerNorm([40, 1])\n",
        "        self.fc1 = nn.Linear(40, 64)\n",
        "        self.ln2 = nn.LayerNorm(64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = self.swish(self.conv1(x))\n",
        "        x = self.swish(self.conv2(x))\n",
        "        x = self.ln1(x).squeeze(2)\n",
        "        x = self.swish(self.fc1(x))\n",
        "        x = self.ln2(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "convnext_model = ConvNeXtInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 15. ViT-inspired\n",
        "class ViTInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, num_heads=8, hidden_dim=128, num_patches=4):\n",
        "        super(ViTInspiredModel, self).__init__()\n",
        "        patch_dim = input_dim // num_patches\n",
        "        self.patch_embed = nn.Linear(patch_dim, hidden_dim)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, hidden_dim))\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x_patched = x.view(batch_size, -1, self.patch_embed.in_features)\n",
        "        x_embed = self.patch_embed(x_patched)\n",
        "        x_embed += self.pos_embed\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x_embed], dim=1)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = x[:, 0]\n",
        "        return self.fc2(x)\n",
        "\n",
        "vit_model = ViTInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 16. ResNet-inspired\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_units, out_units):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_units, out_units)\n",
        "        self.bn1 = nn.BatchNorm1d(out_units)\n",
        "        self.fc2 = nn.Linear(out_units, out_units)\n",
        "        self.bn2 = nn.BatchNorm1d(out_units)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.swish = nn.SiLU()\n",
        "        self.shortcut = nn.Linear(in_units, out_units) if in_units != out_units else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = self.swish(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.bn2(self.fc2(x))\n",
        "        x = x + shortcut\n",
        "        return self.swish(x)\n",
        "\n",
        "class ResNetInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ResNetInspiredModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.res_block1 = ResidualBlock(256, 128)\n",
        "        self.res_block2 = ResidualBlock(128, 128)\n",
        "        self.res_block3 = ResidualBlock(128, 64)\n",
        "        self.res_block4 = ResidualBlock(64, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.swish(self.bn1(self.fc1(x)))\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "        x = self.res_block3(x)\n",
        "        x = self.res_block4(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "resnet_model = ResNetInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# 17. TabNet-inspired\n",
        "class TabNetInspiredModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=128, n_steps=3):\n",
        "        super(TabNetInspiredModel, self).__init__()\n",
        "        self.feature_transformer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.attention = nn.ModuleList([\n",
        "            nn.Linear(hidden_dim, hidden_dim) for _ in range(n_steps)\n",
        "        ])\n",
        "        self.feature_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3)\n",
        "            ) for _ in range(n_steps)\n",
        "        ])\n",
        "        self.final_fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_transformer(x)\n",
        "        for attn, block in zip(self.attention, self.feature_blocks):\n",
        "            attn_weights = F.softmax(attn(x), dim=1)\n",
        "            x = x * attn_weights\n",
        "            x = block(x)\n",
        "            x = self.layer_norm(x)\n",
        "        return self.final_fc(x)\n",
        "\n",
        "tabnet_model = TabNetInspiredModel(input_dim, num_classes).to(device)\n",
        "\n",
        "# Training function with loss tracking\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=100, lr=0.001):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_loss = float('inf')\n",
        "    patience = 10\n",
        "    counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val)\n",
        "        model.train()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"{model.__class__.__name__} Epoch {epoch + 1}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), f'best_{model.__class__.__name__}.pth')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(f'best_{model.__class__.__name__}.pth'))\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "# Train all models and collect losses\n",
        "models = {\n",
        "    'lenet': lenet_model,\n",
        "    'googlenet': googlenet_model,\n",
        "    'alexnet': alexnet_model,\n",
        "    'channel_boosted': channel_boosted_model,\n",
        "    'inception_v2': inception_v2_model,\n",
        "    'inception_v3': inception_v3_model,\n",
        "    'inception_v4': inception_v4_model,\n",
        "    'resnext': resnext_model,\n",
        "    'densenet': densenet_model,\n",
        "    'vgg': vgg_model,\n",
        "    'transformer': transformer_model,\n",
        "    'effnet': effnet_model,\n",
        "    'convnext': convnext_model,\n",
        "    'vit': vit_model,\n",
        "    'resnet': resnet_model,\n",
        "    'tabnet': tabnet_model\n",
        "}\n",
        "\n",
        "accuracies = {}\n",
        "loss_curves = {}\n",
        "for name, model in models.items():\n",
        "    model, train_losses, val_losses = train_model(model, X_train_torch, y_train_torch, X_test_torch, y_test_torch)\n",
        "    loss_curves[name] = (train_losses, val_losses)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_torch)\n",
        "        _, pred_classes = torch.max(outputs, 1)\n",
        "        acc = accuracy_score(y_test, pred_classes.cpu().numpy())\n",
        "        accuracies[name] = acc\n",
        "        print(f\"{name.capitalize()} Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_test, pred_classes.cpu().numpy(), target_names=label_encoder.classes_))\n",
        "    torch.save(model.state_dict(), f'{name}_model.pth')\n",
        "\n",
        "# 18. Ensemble with XGBoost/LightGBM\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "lgb_model = LGBMClassifier(random_state=42, verbose=-1)\n",
        "\n",
        "xgb_params = {'n_estimators': randint(100, 500), 'max_depth': randint(3, 10), 'learning_rate': uniform(0.01, 0.3)}\n",
        "xgb_tuned = RandomizedSearchCV(xgb_model, xgb_params, n_iter=10, cv=5, random_state=42)\n",
        "xgb_tuned.fit(X_train_selected, y_train)\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=[('xgb', xgb_tuned.best_estimator_), ('lgb', lgb_model)],\n",
        "                                    final_estimator=XGBClassifier(random_state=42))\n",
        "stacking_model.fit(X_train_selected, y_train)\n",
        "stacking_pred = stacking_model.predict(X_test_selected)\n",
        "stacking_acc = accuracy_score(y_test, stacking_pred)\n",
        "print(f\"Stacking Ensemble Accuracy: {stacking_acc:.4f}\")\n",
        "print(classification_report(y_test, stacking_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "joblib.dump(stacking_model, 'stacking_model.pkl')\n",
        "\n",
        "# 19. Final Soft Voting Ensemble\n",
        "models_dict = {\n",
        "    'lenet': (lambda x: torch.softmax(lenet_model(x), dim=1).cpu().detach().numpy(), accuracies['lenet']),\n",
        "    'googlenet': (lambda x: torch.softmax(googlenet_model(x), dim=1).cpu().detach().numpy(), accuracies['googlenet']),\n",
        "    'alexnet': (lambda x: torch.softmax(alexnet_model(x), dim=1).cpu().detach().numpy(), accuracies['alexnet']),\n",
        "    'channel_boosted': (lambda x: torch.softmax(channel_boosted_model(x), dim=1).cpu().detach().numpy(), accuracies['channel_boosted']),\n",
        "    'inception_v2': (lambda x: torch.softmax(inception_v2_model(x), dim=1).cpu().detach().numpy(), accuracies['inception_v2']),\n",
        "    'inception_v3': (lambda x: torch.softmax(inception_v3_model(x), dim=1).cpu().detach().numpy(), accuracies['inception_v3']),\n",
        "    'inception_v4': (lambda x: torch.softmax(inception_v4_model(x), dim=1).cpu().detach().numpy(), accuracies['inception_v4']),\n",
        "    'resnext': (lambda x: torch.softmax(resnext_model(x), dim=1).cpu().detach().numpy(), accuracies['resnext']),\n",
        "    'densenet': (lambda x: torch.softmax(densenet_model(x), dim=1).cpu().detach().numpy(), accuracies['densenet']),\n",
        "    'vgg': (lambda x: torch.softmax(vgg_model(x), dim=1).cpu().detach().numpy(), accuracies['vgg']),\n",
        "    'transformer': (lambda x: torch.softmax(transformer_model(x), dim=1).cpu().detach().numpy(), accuracies['transformer']),\n",
        "    'effnet': (lambda x: torch.softmax(effnet_model(x), dim=1).cpu().detach().numpy(), accuracies['effnet']),\n",
        "    'convnext': (lambda x: torch.softmax(convnext_model(x), dim=1).cpu().detach().numpy(), accuracies['convnext']),\n",
        "    'vit': (lambda x: torch.softmax(vit_model(x), dim=1).cpu().detach().numpy(), accuracies['vit']),\n",
        "    'resnet': (lambda x: torch.softmax(resnet_model(x), dim=1).cpu().detach().numpy(), accuracies['resnet']),\n",
        "    'tabnet': (lambda x: torch.softmax(tabnet_model(x), dim=1).cpu().detach().numpy(), accuracies['tabnet']),\n",
        "    'stacking': (lambda x: stacking_model.predict_proba(x), stacking_acc)\n",
        "}\n",
        "\n",
        "total_acc = sum(acc for _, acc in models_dict.values())\n",
        "weights = {name: acc / total_acc for name, (pred_fn, acc) in models_dict.items()}\n",
        "\n",
        "ensemble_probs = np.zeros((len(X_test_scaled), num_classes))\n",
        "for name, (pred_fn, _) in models_dict.items():\n",
        "    if name == 'stacking':\n",
        "        probs = pred_fn(X_test_selected)\n",
        "    else:\n",
        "        probs = pred_fn(X_test_torch)\n",
        "    ensemble_probs += weights[name] * probs\n",
        "\n",
        "final_pred_classes = np.argmax(ensemble_probs, axis=1)\n",
        "final_acc = accuracy_score(y_test, final_pred_classes)\n",
        "print(f\"Final Soft Ensemble Accuracy: {final_acc:.4f}\")\n",
        "print(classification_report(y_test, final_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(VotingClassifier(estimators=[('xgb', xgb_tuned.best_estimator_), ('lgb', lgb_model)], voting='soft'),\n",
        "                            X_train_selected, y_train, cv=5)\n",
        "print(f\"CV Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# 20. Visualizations with Matplotlib\n",
        "plt.style.use('default')  # Use default Matplotlib style\n",
        "\n",
        "# (a) Confusion Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "cm_matrix = confusion_matrix(y_test, final_pred_classes)\n",
        "plt.imshow(cm_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "plt.title('Confusion Matrix - Final Ensemble')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "tick_marks = np.arange(num_classes)\n",
        "plt.xticks(tick_marks, label_encoder.classes_, rotation=45, ha='right')\n",
        "plt.yticks(tick_marks, label_encoder.classes_)\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        plt.text(j, i, cm_matrix[i, j], ha='center', va='center', color='black' if cm_matrix[i, j] < cm_matrix.max() / 2 else 'white')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# (b) Feature Importance Bar Plot\n",
        "mi_scores = mutual_info_classif(X_train, y_train)\n",
        "feature_names = X_train.columns\n",
        "mi_df = pd.DataFrame({'Feature': feature_names, 'MI Score': mi_scores})\n",
        "mi_df = mi_df.sort_values('MI Score', ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(mi_df['Feature'], mi_df['MI Score'], color=plt.cm.viridis(np.linspace(0, 1, len(mi_df))))\n",
        "plt.title('Top 10 Feature Importance (Mutual Information)')\n",
        "plt.xlabel('Mutual Information Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png')\n",
        "plt.close()\n",
        "\n",
        "# (c) Model Accuracy Bar Plot\n",
        "acc_df = pd.DataFrame({\n",
        "    'Model': list(accuracies.keys()) + ['stacking', 'ensemble'],\n",
        "    'Accuracy': list(accuracies.values()) + [stacking_acc, final_acc]\n",
        "})\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(acc_df['Model'], acc_df['Accuracy'], color=plt.cm.magma(np.linspace(0, 1, len(acc_df))))\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Model')\n",
        "plt.xlim(0.95, 1.0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_accuracy.png')\n",
        "plt.close()\n",
        "\n",
        "# (d) Training Loss Curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "for name, (train_losses, val_losses) in loss_curves.items():\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.plot(epochs, train_losses, label=f'{name.capitalize()} Train', linestyle='-')\n",
        "    plt.plot(epochs, val_losses, label=f'{name.capitalize()} Val', linestyle='--')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_curves.png')\n",
        "plt.close()\n",
        "\n",
        "# (e) Per-Class F1 Scores\n",
        "f1_scores = f1_score(y_test, final_pred_classes, average=None)\n",
        "f1_df = pd.DataFrame({\n",
        "    'Class': label_encoder.classes_,\n",
        "    'F1 Score': f1_scores\n",
        "})\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(f1_df['Class'], f1_df['F1 Score'], color=plt.cm.coolwarm(np.linspace(0, 1, len(f1_df))))\n",
        "plt.title('Per-Class F1 Scores - Final Ensemble')\n",
        "plt.xlabel('F1 Score')\n",
        "plt.ylabel('Class')\n",
        "plt.tight_layout()\n",
        "plt.savefig('f1_scores.png')\n",
        "plt.close()\n",
        "\n",
        "# 21. Prediction function (Real-time input and example)\n",
        "def predict_crop(N=None, P=None, K=None, temperature=None, humidity=None, pH=None, rainfall=None):\n",
        "    if N is None:  # Interactive input\n",
        "        print(\"Enter values for crop prediction:\")\n",
        "        N = float(input(\"Nitrogen (N): \"))\n",
        "        P = float(input(\"Phosphorus (P): \"))\n",
        "        K = float(input(\"Potassium (K): \"))\n",
        "        temperature = float(input(\"Temperature (°C): \"))\n",
        "        humidity = float(input(\"Humidity (%): \"))\n",
        "        pH = float(input(\"pH: \"))\n",
        "        rainfall = float(input(\"Rainfall (mm): \"))\n",
        "\n",
        "    input_data = np.array([[N, P, K, temperature, humidity, pH, rainfall]])\n",
        "    input_poly = poly.transform(input_data)\n",
        "    input_selected = selector.transform(input_poly)\n",
        "    input_selected_df = pd.DataFrame(input_selected, columns=selected_features)\n",
        "    input_scaled = scaler.transform(input_selected_df)\n",
        "    input_torch = torch.tensor(input_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    probs = {}\n",
        "    for name, (pred_fn, _) in models_dict.items():\n",
        "        if name == 'stacking':\n",
        "            probs[name] = pred_fn(input_selected_df)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                probs[name] = pred_fn(input_torch)  # Already softmaxed in models_dict\n",
        "\n",
        "    ensemble_prob = np.zeros((1, num_classes))  # Shape (1, 22) for single sample\n",
        "    for name, prob in probs.items():\n",
        "        ensemble_prob += weights[name] * prob\n",
        "\n",
        "    final_class = np.argmax(ensemble_prob, axis=1)\n",
        "    final_crop = label_encoder.inverse_transform(final_class)[0]\n",
        "    individual_preds = [label_encoder.inverse_transform([np.argmax(prob)])[0] for prob in probs.values()]\n",
        "    return final_crop, individual_preds\n",
        "\n",
        "# Example prediction\n",
        "example_input = [90, 42, 43, 20.879744, 82.002744, 6.502985, 202.935536]\n",
        "predicted_crop, individual_preds = predict_crop(*example_input)\n",
        "print(f\"Example Prediction - Recommended Crop: {predicted_crop}\")\n",
        "print(f\"Example Prediction - Individual Model Predictions: {individual_preds}\")\n",
        "\n",
        "# Real-time prediction\n",
        "predicted_crop, individual_preds = predict_crop()\n",
        "print(f\"Real-Time Prediction - Recommended Crop: {predicted_crop}\")\n",
        "print(f\"Real-Time Prediction - Individual Model Predictions: {individual_preds}\")\n",
        "\n",
        "print(\"Visualizations saved: confusion_matrix.png, feature_importance.png, model_accuracy.png, loss_curves.png, f1_scores.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNnGp7m22oXg",
        "outputId": "7b906cb1-ef1e-4500-e602-54cc1cabf83f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'crop-recommendation-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/crop-recommendation-dataset\n",
            "Features after selection: 20\n",
            "LeNetInspiredModel Epoch 10, Loss: 3.0833, Val Loss: 3.0870\n",
            "LeNetInspiredModel Epoch 20, Loss: 3.0621, Val Loss: 3.0624\n",
            "LeNetInspiredModel Epoch 30, Loss: 2.9961, Val Loss: 2.9927\n",
            "LeNetInspiredModel Epoch 40, Loss: 2.8707, Val Loss: 2.8514\n",
            "LeNetInspiredModel Epoch 50, Loss: 2.6673, Val Loss: 2.6458\n",
            "LeNetInspiredModel Epoch 60, Loss: 2.4760, Val Loss: 2.4543\n",
            "LeNetInspiredModel Epoch 70, Loss: 2.3568, Val Loss: 2.3355\n",
            "LeNetInspiredModel Epoch 80, Loss: 2.2939, Val Loss: 2.2750\n",
            "LeNetInspiredModel Epoch 90, Loss: 2.2747, Val Loss: 2.2524\n",
            "LeNetInspiredModel Epoch 100, Loss: 2.2657, Val Loss: 2.2490\n",
            "Lenet Accuracy: 0.2250\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       0.61      1.00      0.75        23\n",
            "      banana       0.00      0.00      0.00        21\n",
            "   blackgram       0.00      0.00      0.00        20\n",
            "    chickpea       0.00      0.00      0.00        26\n",
            "     coconut       0.27      1.00      0.43        27\n",
            "      coffee       0.00      0.00      0.00        17\n",
            "      cotton       0.00      0.00      0.00        17\n",
            "      grapes       0.00      0.00      0.00        14\n",
            "        jute       0.00      0.00      0.00        23\n",
            " kidneybeans       0.14      0.20      0.16        20\n",
            "      lentil       0.08      1.00      0.14        11\n",
            "       maize       0.00      0.00      0.00        21\n",
            "       mango       0.00      0.00      0.00        19\n",
            "   mothbeans       0.00      0.00      0.00        24\n",
            "    mungbean       0.00      0.00      0.00        19\n",
            "   muskmelon       0.42      0.82      0.56        17\n",
            "      orange       0.07      0.14      0.09        14\n",
            "      papaya       0.00      0.00      0.00        23\n",
            "  pigeonpeas       0.25      0.04      0.07        23\n",
            " pomegranate       0.00      0.00      0.00        23\n",
            "        rice       0.41      0.89      0.57        19\n",
            "  watermelon       0.00      0.00      0.00        19\n",
            "\n",
            "    accuracy                           0.23       440\n",
            "   macro avg       0.10      0.23      0.13       440\n",
            "weighted avg       0.11      0.23      0.13       440\n",
            "\n",
            "GoogLeNetInspiredModel Epoch 10, Loss: 3.0000, Val Loss: 2.9866\n",
            "GoogLeNetInspiredModel Epoch 20, Loss: 2.7031, Val Loss: 2.6535\n",
            "GoogLeNetInspiredModel Epoch 30, Loss: 2.2335, Val Loss: 2.1508\n",
            "GoogLeNetInspiredModel Epoch 40, Loss: 1.7763, Val Loss: 1.6078\n",
            "GoogLeNetInspiredModel Epoch 50, Loss: 1.4489, Val Loss: 1.2088\n",
            "GoogLeNetInspiredModel Epoch 60, Loss: 1.2477, Val Loss: 0.9775\n",
            "GoogLeNetInspiredModel Epoch 70, Loss: 1.1285, Val Loss: 0.8600\n",
            "GoogLeNetInspiredModel Epoch 80, Loss: 1.0721, Val Loss: 0.8016\n",
            "GoogLeNetInspiredModel Epoch 90, Loss: 1.0320, Val Loss: 0.7821\n",
            "GoogLeNetInspiredModel Epoch 100, Loss: 1.0336, Val Loss: 0.7790\n",
            "Googlenet Accuracy: 0.7500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       0.78      1.00      0.88        21\n",
            "   blackgram       0.86      0.30      0.44        20\n",
            "    chickpea       0.84      1.00      0.91        26\n",
            "     coconut       0.96      0.89      0.92        27\n",
            "      coffee       0.94      0.94      0.94        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       1.00      0.04      0.08        23\n",
            " kidneybeans       0.84      0.80      0.82        20\n",
            "      lentil       0.36      0.91      0.51        11\n",
            "       maize       1.00      0.24      0.38        21\n",
            "       mango       0.79      1.00      0.88        19\n",
            "   mothbeans       0.81      0.71      0.76        24\n",
            "    mungbean       0.73      1.00      0.84        19\n",
            "   muskmelon       0.81      1.00      0.89        17\n",
            "      orange       0.33      1.00      0.50        14\n",
            "      papaya       0.94      0.70      0.80        23\n",
            "  pigeonpeas       0.88      0.65      0.75        23\n",
            " pomegranate       0.00      0.00      0.00        23\n",
            "        rice       0.45      1.00      0.62        19\n",
            "  watermelon       1.00      0.79      0.88        19\n",
            "\n",
            "    accuracy                           0.75       440\n",
            "   macro avg       0.78      0.77      0.72       440\n",
            "weighted avg       0.80      0.75      0.72       440\n",
            "\n",
            "AlexNetInspiredModel Epoch 10, Loss: 3.0119, Val Loss: 2.9906\n",
            "AlexNetInspiredModel Epoch 20, Loss: 2.5146, Val Loss: 2.4298\n",
            "AlexNetInspiredModel Epoch 30, Loss: 1.7639, Val Loss: 1.4885\n",
            "AlexNetInspiredModel Epoch 40, Loss: 1.2119, Val Loss: 0.8900\n",
            "AlexNetInspiredModel Epoch 50, Loss: 0.8668, Val Loss: 0.6195\n",
            "AlexNetInspiredModel Epoch 60, Loss: 0.7187, Val Loss: 0.4888\n",
            "AlexNetInspiredModel Epoch 70, Loss: 0.6248, Val Loss: 0.4243\n",
            "AlexNetInspiredModel Epoch 80, Loss: 0.5800, Val Loss: 0.3869\n",
            "AlexNetInspiredModel Epoch 90, Loss: 0.5674, Val Loss: 0.3750\n",
            "AlexNetInspiredModel Epoch 100, Loss: 0.5657, Val Loss: 0.3732\n",
            "Alexnet Accuracy: 0.8795\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.91      0.50      0.65        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      0.94      0.97        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       1.00      0.09      0.16        23\n",
            " kidneybeans       0.87      1.00      0.93        20\n",
            "      lentil       0.48      1.00      0.65        11\n",
            "       maize       1.00      0.71      0.83        21\n",
            "       mango       0.90      1.00      0.95        19\n",
            "   mothbeans       0.79      0.79      0.79        24\n",
            "    mungbean       0.95      1.00      0.97        19\n",
            "   muskmelon       0.77      1.00      0.87        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       0.96      1.00      0.98        23\n",
            "  pigeonpeas       0.95      0.78      0.86        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.47      1.00      0.64        19\n",
            "  watermelon       1.00      0.74      0.85        19\n",
            "\n",
            "    accuracy                           0.88       440\n",
            "   macro avg       0.91      0.89      0.87       440\n",
            "weighted avg       0.92      0.88      0.87       440\n",
            "\n",
            "ChannelBoostedCNN Epoch 10, Loss: 2.8521, Val Loss: 2.8068\n",
            "ChannelBoostedCNN Epoch 20, Loss: 2.1120, Val Loss: 1.9708\n",
            "ChannelBoostedCNN Epoch 30, Loss: 1.2946, Val Loss: 1.0434\n",
            "ChannelBoostedCNN Epoch 40, Loss: 0.7644, Val Loss: 0.5469\n",
            "ChannelBoostedCNN Epoch 50, Loss: 0.5049, Val Loss: 0.3531\n",
            "ChannelBoostedCNN Epoch 60, Loss: 0.3969, Val Loss: 0.2736\n",
            "ChannelBoostedCNN Epoch 70, Loss: 0.3154, Val Loss: 0.2353\n",
            "ChannelBoostedCNN Epoch 80, Loss: 0.2907, Val Loss: 0.2180\n",
            "ChannelBoostedCNN Epoch 90, Loss: 0.3090, Val Loss: 0.2114\n",
            "ChannelBoostedCNN Epoch 100, Loss: 0.2763, Val Loss: 0.2107\n",
            "Channel_boosted Accuracy: 0.9295\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.86      0.95      0.90        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.86      0.52      0.65        23\n",
            " kidneybeans       0.95      0.95      0.95        20\n",
            "      lentil       0.73      1.00      0.85        11\n",
            "       maize       0.94      0.71      0.81        21\n",
            "       mango       0.90      1.00      0.95        19\n",
            "   mothbeans       0.95      0.83      0.89        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       0.89      1.00      0.94        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       0.96      1.00      0.98        23\n",
            "  pigeonpeas       0.86      0.83      0.84        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.65      0.89      0.76        19\n",
            "  watermelon       1.00      0.89      0.94        19\n",
            "\n",
            "    accuracy                           0.93       440\n",
            "   macro avg       0.93      0.94      0.93       440\n",
            "weighted avg       0.94      0.93      0.93       440\n",
            "\n",
            "InceptionV2InspiredModel Epoch 10, Loss: 3.0328, Val Loss: 3.0248\n",
            "InceptionV2InspiredModel Epoch 20, Loss: 2.9066, Val Loss: 2.8885\n",
            "InceptionV2InspiredModel Epoch 30, Loss: 2.6657, Val Loss: 2.6162\n",
            "InceptionV2InspiredModel Epoch 40, Loss: 2.3386, Val Loss: 2.2552\n",
            "InceptionV2InspiredModel Epoch 50, Loss: 2.0203, Val Loss: 1.9230\n",
            "InceptionV2InspiredModel Epoch 60, Loss: 1.7916, Val Loss: 1.6759\n",
            "InceptionV2InspiredModel Epoch 70, Loss: 1.6492, Val Loss: 1.5187\n",
            "InceptionV2InspiredModel Epoch 80, Loss: 1.5567, Val Loss: 1.4359\n",
            "InceptionV2InspiredModel Epoch 90, Loss: 1.5631, Val Loss: 1.4046\n",
            "InceptionV2InspiredModel Epoch 100, Loss: 1.5423, Val Loss: 1.3999\n",
            "Inception_v2 Accuracy: 0.6318\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       0.92      1.00      0.96        23\n",
            "      banana       0.83      0.90      0.86        21\n",
            "   blackgram       0.00      0.00      0.00        20\n",
            "    chickpea       1.00      0.96      0.98        26\n",
            "     coconut       1.00      0.96      0.98        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.85      1.00      0.92        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.00      0.00      0.00        23\n",
            " kidneybeans       0.49      1.00      0.66        20\n",
            "      lentil       0.21      1.00      0.35        11\n",
            "       maize       1.00      0.10      0.17        21\n",
            "       mango       0.85      0.58      0.69        19\n",
            "   mothbeans       1.00      0.42      0.59        24\n",
            "    mungbean       0.86      1.00      0.93        19\n",
            "   muskmelon       0.47      1.00      0.64        17\n",
            "      orange       0.37      1.00      0.54        14\n",
            "      papaya       0.00      0.00      0.00        23\n",
            "  pigeonpeas       1.00      0.61      0.76        23\n",
            " pomegranate       0.00      0.00      0.00        23\n",
            "        rice       0.31      1.00      0.47        19\n",
            "  watermelon       0.00      0.00      0.00        19\n",
            "\n",
            "    accuracy                           0.63       440\n",
            "   macro avg       0.60      0.66      0.57       440\n",
            "weighted avg       0.61      0.63      0.56       440\n",
            "\n",
            "InceptionV3InspiredModel Epoch 10, Loss: 2.9467, Val Loss: 2.9343\n",
            "InceptionV3InspiredModel Epoch 20, Loss: 2.5395, Val Loss: 2.5040\n",
            "InceptionV3InspiredModel Epoch 30, Loss: 1.9112, Val Loss: 1.7739\n",
            "InceptionV3InspiredModel Epoch 40, Loss: 1.3808, Val Loss: 1.1273\n",
            "InceptionV3InspiredModel Epoch 50, Loss: 1.0809, Val Loss: 0.7974\n",
            "InceptionV3InspiredModel Epoch 60, Loss: 0.8756, Val Loss: 0.6099\n",
            "InceptionV3InspiredModel Epoch 70, Loss: 0.7259, Val Loss: 0.5138\n",
            "InceptionV3InspiredModel Epoch 80, Loss: 0.6723, Val Loss: 0.4599\n",
            "InceptionV3InspiredModel Epoch 90, Loss: 0.6110, Val Loss: 0.4422\n",
            "InceptionV3InspiredModel Epoch 100, Loss: 0.6322, Val Loss: 0.4392\n",
            "Inception_v3 Accuracy: 0.8636\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.83      1.00      0.91        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       0.96      0.96      0.96        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.79      0.48      0.59        23\n",
            " kidneybeans       0.83      0.95      0.88        20\n",
            "      lentil       0.79      1.00      0.88        11\n",
            "       maize       0.79      0.71      0.75        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       0.84      0.67      0.74        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       0.84      0.94      0.89        17\n",
            "      orange       0.43      0.93      0.59        14\n",
            "      papaya       0.95      0.87      0.91        23\n",
            "  pigeonpeas       0.90      0.78      0.84        23\n",
            " pomegranate       0.88      0.30      0.45        23\n",
            "        rice       0.55      0.84      0.67        19\n",
            "  watermelon       0.94      0.84      0.89        19\n",
            "\n",
            "    accuracy                           0.86       440\n",
            "   macro avg       0.88      0.88      0.86       440\n",
            "weighted avg       0.89      0.86      0.86       440\n",
            "\n",
            "InceptionV4InspiredModel Epoch 10, Loss: 2.8917, Val Loss: 2.8410\n",
            "InceptionV4InspiredModel Epoch 20, Loss: 1.9614, Val Loss: 1.7691\n",
            "InceptionV4InspiredModel Epoch 30, Loss: 1.3780, Val Loss: 1.0083\n",
            "InceptionV4InspiredModel Epoch 40, Loss: 0.8863, Val Loss: 0.6642\n",
            "InceptionV4InspiredModel Epoch 50, Loss: 0.6132, Val Loss: 0.4252\n",
            "InceptionV4InspiredModel Epoch 60, Loss: 0.4561, Val Loss: 0.3042\n",
            "InceptionV4InspiredModel Epoch 70, Loss: 0.3573, Val Loss: 0.2522\n",
            "InceptionV4InspiredModel Epoch 80, Loss: 0.3158, Val Loss: 0.2291\n",
            "InceptionV4InspiredModel Epoch 90, Loss: 0.2997, Val Loss: 0.2185\n",
            "InceptionV4InspiredModel Epoch 100, Loss: 0.3078, Val Loss: 0.2168\n",
            "Inception_v4 Accuracy: 0.9409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.79      0.95      0.86        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      0.94      0.97        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.79      1.00      0.88        23\n",
            " kidneybeans       0.91      1.00      0.95        20\n",
            "      lentil       0.65      1.00      0.79        11\n",
            "       maize       0.84      0.76      0.80        21\n",
            "       mango       0.90      1.00      0.95        19\n",
            "   mothbeans       0.94      0.67      0.78        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      0.74      0.85        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       1.00      0.74      0.85        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.94       440\n",
            "   macro avg       0.94      0.95      0.94       440\n",
            "weighted avg       0.95      0.94      0.94       440\n",
            "\n",
            "ResNeXtInspiredModel Epoch 10, Loss: 1.8356, Val Loss: 3.0778\n",
            "ResNeXtInspiredModel Epoch 20, Loss: 0.8803, Val Loss: 2.9360\n",
            "ResNeXtInspiredModel Epoch 30, Loss: 0.3657, Val Loss: 2.0840\n",
            "ResNeXtInspiredModel Epoch 40, Loss: 0.1831, Val Loss: 0.6159\n",
            "ResNeXtInspiredModel Epoch 50, Loss: 0.1151, Val Loss: 0.1512\n",
            "ResNeXtInspiredModel Epoch 60, Loss: 0.0952, Val Loss: 0.0783\n",
            "ResNeXtInspiredModel Epoch 70, Loss: 0.0792, Val Loss: 0.0664\n",
            "ResNeXtInspiredModel Epoch 80, Loss: 0.0776, Val Loss: 0.0613\n",
            "ResNeXtInspiredModel Epoch 90, Loss: 0.0712, Val Loss: 0.0606\n",
            "ResNeXtInspiredModel Epoch 100, Loss: 0.0683, Val Loss: 0.0603\n",
            "Resnext Accuracy: 0.9818\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      0.95      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.88      0.91      0.89        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      1.00      1.00        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      0.91      0.95        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.94      0.84      0.89        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.98       440\n",
            "   macro avg       0.98      0.98      0.98       440\n",
            "weighted avg       0.98      0.98      0.98       440\n",
            "\n",
            "DenseNetInspiredModel Epoch 10, Loss: 2.7128, Val Loss: 3.0008\n",
            "DenseNetInspiredModel Epoch 20, Loss: 2.3646, Val Loss: 2.8143\n",
            "DenseNetInspiredModel Epoch 30, Loss: 1.9923, Val Loss: 2.4966\n",
            "DenseNetInspiredModel Epoch 40, Loss: 1.6642, Val Loss: 2.0606\n",
            "DenseNetInspiredModel Epoch 50, Loss: 1.3967, Val Loss: 1.6237\n",
            "DenseNetInspiredModel Epoch 60, Loss: 1.2244, Val Loss: 1.2871\n",
            "DenseNetInspiredModel Epoch 70, Loss: 1.0863, Val Loss: 1.0759\n",
            "DenseNetInspiredModel Epoch 80, Loss: 1.0285, Val Loss: 0.9656\n",
            "DenseNetInspiredModel Epoch 90, Loss: 0.9988, Val Loss: 0.9173\n",
            "DenseNetInspiredModel Epoch 100, Loss: 1.0072, Val Loss: 0.9009\n",
            "Densenet Accuracy: 0.9432\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.87      1.00      0.93        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      0.94      0.97        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.84      0.70      0.76        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.83      0.91      0.87        11\n",
            "       maize       1.00      0.81      0.89        21\n",
            "       mango       0.90      1.00      0.95        19\n",
            "   mothbeans       0.92      0.92      0.92        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       0.85      1.00      0.92        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       0.96      0.96      0.96        23\n",
            "  pigeonpeas       1.00      0.87      0.93        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.67      0.84      0.74        19\n",
            "  watermelon       1.00      0.84      0.91        19\n",
            "\n",
            "    accuracy                           0.94       440\n",
            "   macro avg       0.95      0.94      0.94       440\n",
            "weighted avg       0.95      0.94      0.94       440\n",
            "\n",
            "VGGInspiredModel Epoch 10, Loss: 3.0787, Val Loss: 3.0832\n",
            "VGGInspiredModel Epoch 20, Loss: 2.9943, Val Loss: 2.9838\n",
            "VGGInspiredModel Epoch 30, Loss: 2.6040, Val Loss: 2.5750\n",
            "VGGInspiredModel Epoch 40, Loss: 2.0620, Val Loss: 1.9651\n",
            "VGGInspiredModel Epoch 50, Loss: 1.6407, Val Loss: 1.4868\n",
            "VGGInspiredModel Epoch 60, Loss: 1.4241, Val Loss: 1.2331\n",
            "VGGInspiredModel Epoch 70, Loss: 1.2926, Val Loss: 1.1216\n",
            "VGGInspiredModel Epoch 80, Loss: 1.2188, Val Loss: 1.0728\n",
            "VGGInspiredModel Epoch 90, Loss: 1.2238, Val Loss: 1.0541\n",
            "VGGInspiredModel Epoch 100, Loss: 1.2047, Val Loss: 1.0511\n",
            "Vgg Accuracy: 0.5795\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       0.79      1.00      0.88        23\n",
            "      banana       0.00      0.00      0.00        21\n",
            "   blackgram       0.00      0.00      0.00        20\n",
            "    chickpea       0.59      0.38      0.47        26\n",
            "     coconut       0.76      0.96      0.85        27\n",
            "      coffee       0.86      0.71      0.77        17\n",
            "      cotton       1.00      0.65      0.79        17\n",
            "      grapes       0.39      1.00      0.56        14\n",
            "        jute       0.00      0.00      0.00        23\n",
            " kidneybeans       0.39      0.70      0.50        20\n",
            "      lentil       0.28      1.00      0.43        11\n",
            "       maize       1.00      0.14      0.25        21\n",
            "       mango       0.73      0.84      0.78        19\n",
            "   mothbeans       0.63      0.71      0.67        24\n",
            "    mungbean       0.73      1.00      0.84        19\n",
            "   muskmelon       0.50      1.00      0.67        17\n",
            "      orange       0.38      1.00      0.55        14\n",
            "      papaya       0.87      0.57      0.68        23\n",
            "  pigeonpeas       0.76      0.57      0.65        23\n",
            " pomegranate       0.00      0.00      0.00        23\n",
            "        rice       0.47      0.95      0.63        19\n",
            "  watermelon       1.00      0.21      0.35        19\n",
            "\n",
            "    accuracy                           0.58       440\n",
            "   macro avg       0.55      0.61      0.51       440\n",
            "weighted avg       0.56      0.58      0.51       440\n",
            "\n",
            "TransformerInspiredModel Epoch 10, Loss: 2.0219, Val Loss: 1.8555\n",
            "TransformerInspiredModel Epoch 20, Loss: 1.2561, Val Loss: 1.0699\n",
            "TransformerInspiredModel Epoch 30, Loss: 0.7225, Val Loss: 0.5458\n",
            "TransformerInspiredModel Epoch 40, Loss: 0.4175, Val Loss: 0.2711\n",
            "TransformerInspiredModel Epoch 50, Loss: 0.2716, Val Loss: 0.1727\n",
            "TransformerInspiredModel Epoch 60, Loss: 0.1858, Val Loss: 0.1251\n",
            "TransformerInspiredModel Epoch 70, Loss: 0.1677, Val Loss: 0.1044\n",
            "TransformerInspiredModel Epoch 80, Loss: 0.1468, Val Loss: 0.0948\n",
            "TransformerInspiredModel Epoch 90, Loss: 0.1522, Val Loss: 0.0940\n",
            "TransformerInspiredModel Epoch 100, Loss: 0.1481, Val Loss: 0.0936\n",
            "Transformer Accuracy: 0.9795\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      0.95      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.84      0.91      0.88        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      1.00      1.00        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      0.91      0.95        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.94      0.79      0.86        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.98       440\n",
            "   macro avg       0.98      0.98      0.98       440\n",
            "weighted avg       0.98      0.98      0.98       440\n",
            "\n",
            "EfficientNetInspiredModel Epoch 10, Loss: 2.0453, Val Loss: 1.8550\n",
            "EfficientNetInspiredModel Epoch 20, Loss: 1.6114, Val Loss: 1.3136\n",
            "EfficientNetInspiredModel Epoch 30, Loss: 1.3427, Val Loss: 0.9879\n",
            "EfficientNetInspiredModel Epoch 40, Loss: 1.1453, Val Loss: 0.8075\n",
            "EfficientNetInspiredModel Epoch 50, Loss: 1.0213, Val Loss: 0.6994\n",
            "EfficientNetInspiredModel Epoch 60, Loss: 0.9351, Val Loss: 0.6295\n",
            "EfficientNetInspiredModel Epoch 70, Loss: 0.8677, Val Loss: 0.5881\n",
            "EfficientNetInspiredModel Epoch 80, Loss: 0.8585, Val Loss: 0.5672\n",
            "EfficientNetInspiredModel Epoch 90, Loss: 0.8393, Val Loss: 0.5603\n",
            "EfficientNetInspiredModel Epoch 100, Loss: 0.8310, Val Loss: 0.5607\n",
            "Effnet Accuracy: 0.9614\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.90      0.95      0.93        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.81      0.96      0.88        23\n",
            " kidneybeans       0.95      0.95      0.95        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       0.86      1.00      0.93        19\n",
            "   mothbeans       1.00      0.88      0.93        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      0.87      0.93        23\n",
            "  pigeonpeas       0.95      0.87      0.91        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.82      0.74      0.78        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.96       440\n",
            "   macro avg       0.96      0.96      0.96       440\n",
            "weighted avg       0.96      0.96      0.96       440\n",
            "\n",
            "ConvNeXtInspiredModel Epoch 10, Loss: 2.6797, Val Loss: 2.6127\n",
            "ConvNeXtInspiredModel Epoch 20, Loss: 2.1465, Val Loss: 2.0554\n",
            "ConvNeXtInspiredModel Epoch 30, Loss: 1.6604, Val Loss: 1.5815\n",
            "ConvNeXtInspiredModel Epoch 40, Loss: 1.3811, Val Loss: 1.3135\n",
            "ConvNeXtInspiredModel Epoch 50, Loss: 1.2013, Val Loss: 1.1417\n",
            "ConvNeXtInspiredModel Epoch 60, Loss: 1.0858, Val Loss: 1.0359\n",
            "ConvNeXtInspiredModel Epoch 70, Loss: 1.0219, Val Loss: 0.9767\n",
            "ConvNeXtInspiredModel Epoch 80, Loss: 0.9815, Val Loss: 0.9464\n",
            "ConvNeXtInspiredModel Epoch 90, Loss: 0.9813, Val Loss: 0.9349\n",
            "ConvNeXtInspiredModel Epoch 100, Loss: 0.9750, Val Loss: 0.9331\n",
            "Convnext Accuracy: 0.8705\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.89      0.80      0.84        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       0.93      0.96      0.95        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.89      1.00      0.94        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.69      0.48      0.56        23\n",
            " kidneybeans       0.77      0.85      0.81        20\n",
            "      lentil       0.52      1.00      0.69        11\n",
            "       maize       1.00      0.90      0.95        21\n",
            "       mango       0.73      1.00      0.84        19\n",
            "   mothbeans       1.00      0.21      0.34        24\n",
            "    mungbean       0.90      1.00      0.95        19\n",
            "   muskmelon       1.00      0.88      0.94        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       0.95      0.83      0.88        23\n",
            "  pigeonpeas       0.87      0.87      0.87        23\n",
            " pomegranate       0.95      0.91      0.93        23\n",
            "        rice       0.50      0.74      0.60        19\n",
            "  watermelon       0.90      1.00      0.95        19\n",
            "\n",
            "    accuracy                           0.87       440\n",
            "   macro avg       0.88      0.88      0.86       440\n",
            "weighted avg       0.89      0.87      0.86       440\n",
            "\n",
            "ViTInspiredModel Epoch 10, Loss: 2.7267, Val Loss: 2.6582\n",
            "ViTInspiredModel Epoch 20, Loss: 1.8699, Val Loss: 1.7532\n",
            "ViTInspiredModel Epoch 30, Loss: 1.1549, Val Loss: 1.0334\n",
            "ViTInspiredModel Epoch 40, Loss: 0.6956, Val Loss: 0.6046\n",
            "ViTInspiredModel Epoch 50, Loss: 0.4562, Val Loss: 0.3913\n",
            "ViTInspiredModel Epoch 60, Loss: 0.3317, Val Loss: 0.2903\n",
            "ViTInspiredModel Epoch 70, Loss: 0.2856, Val Loss: 0.2429\n",
            "ViTInspiredModel Epoch 80, Loss: 0.2487, Val Loss: 0.2237\n",
            "ViTInspiredModel Epoch 90, Loss: 0.2423, Val Loss: 0.2166\n",
            "ViTInspiredModel Epoch 100, Loss: 0.2397, Val Loss: 0.2155\n",
            "Vit Accuracy: 0.9545\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      0.95      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.81      0.74      0.77        23\n",
            " kidneybeans       0.95      0.95      0.95        20\n",
            "      lentil       0.73      1.00      0.85        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.88      0.93        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       0.93      1.00      0.97        14\n",
            "      papaya       1.00      0.96      0.98        23\n",
            "  pigeonpeas       0.95      0.91      0.93        23\n",
            " pomegranate       1.00      0.96      0.98        23\n",
            "        rice       0.71      0.79      0.75        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.95       440\n",
            "   macro avg       0.95      0.96      0.95       440\n",
            "weighted avg       0.96      0.95      0.96       440\n",
            "\n",
            "ResNetInspiredModel Epoch 10, Loss: 2.0079, Val Loss: 2.9418\n",
            "ResNetInspiredModel Epoch 20, Loss: 1.1496, Val Loss: 1.7111\n",
            "ResNetInspiredModel Epoch 30, Loss: 0.6626, Val Loss: 0.5484\n",
            "ResNetInspiredModel Epoch 40, Loss: 0.4203, Val Loss: 0.2624\n",
            "ResNetInspiredModel Epoch 50, Loss: 0.2957, Val Loss: 0.1477\n",
            "ResNetInspiredModel Epoch 60, Loss: 0.2339, Val Loss: 0.0996\n",
            "ResNetInspiredModel Epoch 70, Loss: 0.1867, Val Loss: 0.0829\n",
            "ResNetInspiredModel Epoch 80, Loss: 0.1934, Val Loss: 0.0798\n",
            "ResNetInspiredModel Epoch 90, Loss: 0.1641, Val Loss: 0.0761\n",
            "ResNetInspiredModel Epoch 100, Loss: 0.1847, Val Loss: 0.0748\n",
            "Resnet Accuracy: 0.9705\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      0.95      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.80      0.87      0.83        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.79      1.00      0.88        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.92      0.96        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      0.91      0.95        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.88      0.74      0.80        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.97      0.97      0.97       440\n",
            "weighted avg       0.97      0.97      0.97       440\n",
            "\n",
            "TabNetInspiredModel Epoch 10, Loss: 1.7883, Val Loss: 3.4551\n",
            "Tabnet Accuracy: 0.0386\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       0.00      0.00      0.00        23\n",
            "      banana       0.00      0.00      0.00        21\n",
            "   blackgram       0.00      0.00      0.00        20\n",
            "    chickpea       0.00      0.00      0.00        26\n",
            "     coconut       0.00      0.00      0.00        27\n",
            "      coffee       0.00      0.00      0.00        17\n",
            "      cotton       0.00      0.00      0.00        17\n",
            "      grapes       0.00      0.00      0.00        14\n",
            "        jute       0.00      0.00      0.00        23\n",
            " kidneybeans       0.00      0.00      0.00        20\n",
            "      lentil       0.00      0.00      0.00        11\n",
            "       maize       0.00      0.00      0.00        21\n",
            "       mango       0.00      0.00      0.00        19\n",
            "   mothbeans       0.00      0.00      0.00        24\n",
            "    mungbean       0.00      0.00      0.00        19\n",
            "   muskmelon       0.04      1.00      0.07        17\n",
            "      orange       0.00      0.00      0.00        14\n",
            "      papaya       0.00      0.00      0.00        23\n",
            "  pigeonpeas       0.00      0.00      0.00        23\n",
            " pomegranate       0.00      0.00      0.00        23\n",
            "        rice       0.00      0.00      0.00        19\n",
            "  watermelon       0.00      0.00      0.00        19\n",
            "\n",
            "    accuracy                           0.04       440\n",
            "   macro avg       0.00      0.05      0.00       440\n",
            "weighted avg       0.00      0.04      0.00       440\n",
            "\n",
            "Stacking Ensemble Accuracy: 0.9795\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      1.00      0.98        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.89      1.00      0.94        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.91      0.87      0.89        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.96      0.98        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.96      0.96      0.96        23\n",
            " pomegranate       0.96      1.00      0.98        23\n",
            "        rice       0.94      0.89      0.92        19\n",
            "  watermelon       1.00      0.95      0.97        19\n",
            "\n",
            "    accuracy                           0.98       440\n",
            "   macro avg       0.98      0.98      0.98       440\n",
            "weighted avg       0.98      0.98      0.98       440\n",
            "\n",
            "Final Soft Ensemble Accuracy: 0.9773\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      0.95      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.91      0.91      0.91        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.73      1.00      0.85        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.88      0.93        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      0.91      0.95        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.94      0.89      0.92        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.98       440\n",
            "   macro avg       0.97      0.98      0.98       440\n",
            "weighted avg       0.98      0.98      0.98       440\n",
            "\n",
            "CV Mean Accuracy: 0.9869 (+/- 0.0137)\n",
            "Example Prediction - Recommended Crop: rice\n",
            "Example Prediction - Individual Model Predictions: ['rice', 'rice', 'rice', 'rice', 'rice', 'rice', 'jute', 'rice', 'rice', 'rice', 'rice', 'rice', 'jute', 'rice', 'rice', 'muskmelon', 'rice']\n",
            "Enter values for crop prediction:\n",
            "Nitrogen (N): 86\n",
            "Phosphorus (P): 43\n",
            "Potassium (K): 45\n",
            "Temperature (°C): 44\n",
            "Humidity (%): 46\n",
            "pH: 7\n",
            "Rainfall (mm): 100\n",
            "Real-Time Prediction - Recommended Crop: mango\n",
            "Real-Time Prediction - Individual Model Predictions: ['orange', 'chickpea', 'pigeonpeas', 'coffee', 'kidneybeans', 'pigeonpeas', 'banana', 'coffee', 'coffee', 'pigeonpeas', 'mango', 'coffee', 'chickpea', 'coffee', 'mango', 'muskmelon', 'mango']\n",
            "Visualizations saved: confusion_matrix.png, feature_importance.png, model_accuracy.png, loss_curves.png, f1_scores.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22JlkMMu4Ph9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}